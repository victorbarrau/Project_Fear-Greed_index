{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victorbarrau/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "daily_tweet/BNB/2023-03-11.csv créé avec succès.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'Text'] = df['Text'].apply(lambda x: url_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: hashtag_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: mention_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: digit_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'],inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:122: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "daily_tweet/Bitcoin/2023-03-11.csv créé avec succès.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'Text'] = df['Text'].apply(lambda x: url_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: hashtag_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: mention_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: digit_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'],inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:122: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_tweet/Ethereum/2023-03-11.csv créé avec succès.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'Text'] = df['Text'].apply(lambda x: url_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: hashtag_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: mention_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: digit_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'],inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:122: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_tweet/Dogecoin/2023-03-11.csv créé avec succès.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'Text'] = df['Text'].apply(lambda x: url_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: hashtag_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: mention_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Text'] = df['Text'].apply(lambda x: digit_pattern.sub('', x))\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(subset=['Text'],inplace=True)\n",
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2310911435.py:122: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import google.oauth2.service_account\n",
    "import nltk\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from google.cloud import bigquery\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import google.oauth2.service_account\n",
    "#from google.cloud import bigquery\n",
    "\n",
    "start_time = time.time()\n",
    "#creds = google.oauth2.service_account.Credentials.from_service_account_file('/home/code/clef/clef.json',scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/code/clef/clef.json\"\n",
    "\n",
    "start_date = datetime.date.today()  -  datetime.timedelta(days=4)\n",
    "end_date = datetime.date.today() - datetime.timedelta(days=3)\n",
    "\n",
    "date=\"since:{} until:{}\".format(start_date, end_date )\n",
    "client2 = bigquery.Client()\n",
    "client = storage.Client()\n",
    "bucket_name = \"data_tweet\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Fonction pour écrire les tweets dans un fichier CSV et les stocker dans le bucket Google Cloud Storage\n",
    "def write_tweets_to_file(tweets,date,crypto):\n",
    "    # Création d'un dataframe Pandas à partir des tweets\n",
    "    df = pd.DataFrame(tweets, columns=['Datetime', 'Text','retweet','like','lang'])\n",
    "    # Filtrage des tweets en anglais\n",
    "    df = df[df['lang'] == 'en']\n",
    "    date_of_df=date[6:16]\n",
    "    df['date'] = pd.to_datetime(df['Datetime']).dt.strftime('%Y-%m-%d')\n",
    "    df=df[df['date']==date_of_df]\n",
    "    csv_data = df.to_csv(index=False)\n",
    "    file_name = \"daily_tweet/\"+crypto+\"/\" + str(date_of_df)+\".csv\"\n",
    "    bucket.blob(file_name).upload_from_string(csv_data)\n",
    "    print(f\"{file_name} créé avec succès.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_tweet(crypto,nb_tweet,date):\n",
    "    tweets_list=[]\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(crypto+' '+str(date) ).get_items()):\n",
    "        if i>nb_tweet:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.rawContent,tweet.retweetCount,tweet.likeCount,tweet.lang])\n",
    "\n",
    "    df=write_tweets_to_file(tweets_list,date,crypto)\n",
    "    return df\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "ban_words = [  'secret formula','FREE' , 'private access',  'limited spots',  'limited seats',   'make money online',   'be your own boss','easy work',  'earn cash',    'automated system','instant cash',  'quick cash',  'easy cash',    'instant success', 'ðŸ','guaranteed success',  'proven system',  'no experience required',  'no skills needed''no work required',  'no risk',  'zero risk',  'risk-free',  'no investment required','no upfront cost',  'no catch',  'no strings attached',  'limited time offer' ,'Ÿ','last chance',  'exclusive offer',  'special offer',  'secret offer',  'invite only','pre-launch',    'secret strategy',  'hidden strategy', 'secret loophole',  'hidden loophole',  'insider secrets',  'insider info',  'inside knowledge','miracle cure',  'miracle product',  'amazing results',  'instant results',  'mind-blowing results','too good to be true',  'millionaire overnight',  'instant millionaire',  'financial miracle', 'financial breakthrough',  'money miracle',  'money breakthrough',  'secret method',  'hidden method', 'secret technique',  'hidden technique',  'secret formula',  'hidden formula',  'secret blueprint','™','hidden blueprint',  'secret system',  'hidden system',  'secret software',  'hidden software', 'secret tool',  'hidden tool',  'secret strategy',  'hidden strategy',  'secret loophole',  'hidden loophole',  'unlimited income potential','link','TRX']\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "date_generated = [date.strftime(\"%Y-%m-%d\") for date in [start_date + datetime.timedelta(days=x) for x in range((end_date-start_date).days+1)]]\n",
    "\n",
    "def filter_and_annalyse(df,ban_words=ban_words):\n",
    "\n",
    "    mask = df['Text'].str.contains('|'.join(ban_words), case=False)\n",
    "    df = df[~mask]\n",
    "\n",
    "    url_pattern = re.compile(r'https?://\\S+')\n",
    "    hashtag_pattern = re.compile(r'#\\w+')\n",
    "    mention_pattern = re.compile(r'@\\w+')\n",
    "    digit_pattern = re.compile(r'\\b\\d+\\b')\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "    df.loc[:, 'Text'] = df['Text'].apply(lambda x: url_pattern.sub('', x))\n",
    "\n",
    "    df['Text'] = df['Text'].apply(lambda x: hashtag_pattern.sub('', x))\n",
    "    df['Text'] = df['Text'].apply(lambda x: mention_pattern.sub('', x))\n",
    "    df['Text'] = df['Text'].apply(lambda x: digit_pattern.sub('', x))\n",
    "    df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
    "\n",
    "    df.drop_duplicates(subset=['Text'],inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_sentiment(df,date,crypto):\n",
    "\n",
    "    column = df['Text']\n",
    "    column = column.apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "    df['Text'] = column\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/Finbert\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/Finbert\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        input_ids = torch.tensor(tokenizer.encode(row['Text'], add_special_tokens=True)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[0]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        sentiment = probs[0][1].item() # La probabilité de la classe positive est utilisée pour la note sur 10\n",
    "        df.loc[i, 'sentiment'] = sentiment\n",
    "\n",
    "    csv_data = df.to_csv(index=False)\n",
    "    date=date[6:16]\n",
    "    file_name2 = \"predictions/\"+crypto+\"/finbert_ML_pred_\" + str(date) +\".csv\"\n",
    "    bucket.blob(file_name2).upload_from_string(csv_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def interpreatiton_normalisation(df,crypto):\n",
    "    from datetime import datetime, timedelta\n",
    "    df = df.groupby(['date']).mean()\n",
    "    if crypto == \"Bitcoin\":\n",
    "        max_sentiment =0.18867444594931193\n",
    "        min_sentiment =0.06760287501519527\n",
    "        max_sentiment_reverse=0.1885925980865344\n",
    "        min_sentiment_reverse=0.18844601483364987\n",
    "    elif crypto == \"Ethereum\":\n",
    "        max_sentiment = 0.17579640225524298\n",
    "        min_sentiment = 0.049215803295236206\n",
    "        max_sentiment_reverse=0.1757341045966489\n",
    "        min_sentiment_reverse=0.17557387811631814\n",
    "    elif crypto == \"BNB\":\n",
    "        max_sentiment = 0.15699728353315043\n",
    "        min_sentiment = 0.03457914667716013\n",
    "        max_sentiment_reverse=0.15695495238604754\n",
    "        min_sentiment_reverse=0.15680509038373464\n",
    "    elif crypto == \"Dogecoin\":\n",
    "        max_sentiment = 0.15094389359536597\n",
    "        min_sentiment = 0.03631142479017236\n",
    "        max_sentiment_reverse=0.15090226891267067\n",
    "        min_sentiment_reverse=0.15077086288362693\n",
    "\n",
    "    df['sentiment'] = (100 - df['sentiment']) * (max_sentiment - min_sentiment) / 100 + min_sentiment\n",
    "\n",
    "    df['sentiment'] = (df['sentiment'] - min_sentiment_reverse) / (max_sentiment_reverse - min_sentiment_reverse) * 100\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].clip(lower=0, upper=100)\n",
    "\n",
    "    df.loc[df[\"sentiment\"] < 0, \"sentiment\"] = 0\n",
    "    df.loc[df[\"sentiment\"] > 100, \"sentiment\"] = 100\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "client2 = bigquery.Client()\n",
    "\n",
    "def ingestion_sentiment(df,crypto):\n",
    "    print('icic3')\n",
    "    project = 'sublime-vial-365809'\n",
    "    df = df.loc[:, ['date', 'sentiment']]\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"date\", bigquery.enums.SqlTypeNames.DATE),\n",
    "            bigquery.SchemaField(\"sentiment\", bigquery.enums.SqlTypeNames.FLOAT),\n",
    "        ],\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "    )\n",
    "    \n",
    "    table_id = 'sentiments.'+crypto\n",
    "    job = client2.load_table_from_dataframe(\n",
    "        df, table_id, job_config=job_config\n",
    "    )\n",
    "    job.result()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def do_the_shit(crypto,nb_tweet,date):\n",
    "    df = get_tweet(crypto,nb_tweet,date)\n",
    "    df = filter_and_annalyse(df)\n",
    "    df = analyze_sentiment(df,date,crypto)\n",
    "    df = interpreatiton_normalisation(df,crypto)\n",
    "    #ingestion_sentiment(df,crypto)\n",
    "\n",
    "print(do_the_shit('BNB',2000,date))\n",
    "do_the_shit('Bitcoin',5500,date)\n",
    "do_the_shit('Ethereum',3500,date)\n",
    "do_the_shit('Dogecoin',2000,date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import google.oauth2.service_account\n",
    "import nltk\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from google.cloud import bigquery\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import google.oauth2.service_account\n",
    "#from google.cloud import bigquery\n",
    "\n",
    "\n",
    "#creds = google.oauth2.service_account.Credentials.from_service_account_file('/home/code/clef/clef.json',scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/code/clef/clef.json\"\n",
    "\n",
    "start_date = datetime.date.today()  -  datetime.timedelta(days=3)\n",
    "end_date = datetime.date.today()\n",
    "\n",
    "date=\"since:{} until:{}\".format(start_date, end_date )\n",
    "#client2 = bigquery.Client()\n",
    "#client = storage.Client(credentials=creds)\n",
    "client = storage.Client()\n",
    "bucket_name = \"data_tweet\"\n",
    "bucket = client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'since:2023-03-12 until:2023-03-15'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "icic\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "icic2\n",
      "icic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2185407889.py:29: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icic\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "icic2\n",
      "icic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2185407889.py:29: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icic\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "icic2\n",
      "icic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2185407889.py:29: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icic\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "icic2\n",
      "icic3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/1j2lys1s0g344s_0q1rbvvt00000gn/T/ipykernel_51875/2185407889.py:29: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['date']).mean()\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import io\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client2 = bigquery.Client()\n",
    "client = storage.Client()\n",
    "bucket_name = \"data_tweet\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "def get_csv_pred(crypto):\n",
    "    print('icic')\n",
    "    date = datetime.date.today()  -  datetime.timedelta(days=5)\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket('data_tweet')\n",
    "    file_name = \"predictions/\"+crypto+\"/finbert_ML_pred_\" + str(date) +\".csv\"\n",
    "    blob = bucket.get_blob(file_name)\n",
    "    content = blob.download_as_string()\n",
    "    df = pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
    "    df['date'] = pd.to_datetime(date, format='%Y-%m-%d')\n",
    "    return df\n",
    "    \n",
    "\n",
    "def interpreatiton_normalisation(df,crypto):\n",
    "    from datetime import datetime, timedelta\n",
    "    print('icic2')\n",
    "\n",
    "    df = df.groupby(['date']).mean()\n",
    "    if crypto == \"Bitcoin\":\n",
    "        max_sentiment =0.18867444594931193\n",
    "        min_sentiment =0.06760287501519527\n",
    "        max_sentiment_reverse=0.1885925980865344\n",
    "        min_sentiment_reverse=0.18844601483364987\n",
    "    elif crypto == \"Ethereum\":\n",
    "        max_sentiment = 0.17579640225524298\n",
    "        min_sentiment = 0.049215803295236206\n",
    "        max_sentiment_reverse=0.1757341045966489\n",
    "        min_sentiment_reverse=0.17557387811631814\n",
    "    elif crypto == \"BNB\":\n",
    "        max_sentiment = 0.15699728353315043\n",
    "        min_sentiment = 0.03457914667716013\n",
    "        max_sentiment_reverse=0.15695495238604754\n",
    "        min_sentiment_reverse=0.15680509038373464\n",
    "    elif crypto == \"Dogecoin\":\n",
    "        max_sentiment = 0.15094389359536597\n",
    "        min_sentiment = 0.03631142479017236\n",
    "        max_sentiment_reverse=0.15090226891267067\n",
    "        min_sentiment_reverse=0.15077086288362693\n",
    "\n",
    "    df['sentiment'] = (100 - df['sentiment']) * (max_sentiment - min_sentiment) / 100 + min_sentiment\n",
    "\n",
    "    df['sentiment'] = (df['sentiment'] - min_sentiment_reverse) / (max_sentiment_reverse - min_sentiment_reverse) * 100\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].clip(lower=0, upper=100)\n",
    "\n",
    "    df.loc[df[\"sentiment\"] < 0, \"sentiment\"] = 0\n",
    "    df.loc[df[\"sentiment\"] > 100, \"sentiment\"] = 100\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "def ingestion_sentiment(df,crypto):\n",
    "    print('icic3')\n",
    "    project = 'sublime-vial-365809'\n",
    "    df = df.loc[:, ['date', 'sentiment']]\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"date\", bigquery.enums.SqlTypeNames.DATE),\n",
    "            bigquery.SchemaField(\"sentiment\", bigquery.enums.SqlTypeNames.FLOAT),\n",
    "        ],\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "    )\n",
    "    \n",
    "    table_id = 'sentiments.'+crypto\n",
    "    job = client2.load_table_from_dataframe(\n",
    "        df, table_id, job_config=job_config\n",
    "    )\n",
    "    job.result()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def do_the_shit(crypto):\n",
    "\n",
    "    df = get_csv_pred(crypto)\n",
    "    df = interpreatiton_normalisation(df,crypto)\n",
    "    ingestion_sentiment(df,crypto)\n",
    "\n",
    "\n",
    "do_the_shit('BNB')\n",
    "do_the_shit('Bitcoin')\n",
    "do_the_shit('Ethereum')\n",
    "do_the_shit('Dogecoin')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetFandG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
